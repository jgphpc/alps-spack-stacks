{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#cscs-user-environments","title":"CSCS User Environments","text":"<p>Documentation for the user-environments provided on CSCS Alps infrastructure.</p>"},{"location":"pkg-application-tutorial/","title":"Application Packaging Tutorial","text":"<p>target audience</p> <p>The target audience for this tutorial is CSCS staff who want to provide an application or programming enivoronment uenv for users on Alps. We assume that you are familiar with Spack and how to build uenv using Stackinator - the focus here will be on best practices.</p> <p>This tutorial walks through configuring and maintaining a uenv recipe and deployment a representatative HPC application. The uenv has to support some commone use cases, and should cover most of the aspects of deploying your own uenv.</p> <p>Arbor is scientific software for neuroscience simulation, with features including:</p> <ul> <li>A C++ library with a Python interface</li> <li>Distributed execution through MPI</li> <li>Multicore simulation</li> <li>Support for both NVIDIA and AMD GPUs</li> </ul>"},{"location":"pkg-application-tutorial/#requirements","title":"Requirements","text":"<p>Before starting, we gather requirements for the use cases of the uenv on the system, in order to understand:</p> <ul> <li>which packages the uenv will provide;</li> <li>which interfaces the uenv will provide to those packages.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-workflows","title":"Supported workflows","text":"<p>For Arbor we wish to support two workflows:</p> <ul> <li>Application: provide optimised builds of Arbor for users to use directly</li> <li>BYO/Developer: Arbor is under active development, and some users require the ability to build the latest bleeding edge version, or build with a customised configuration. This workflow also supports Arbor developers, who need to implement new features, fix bugs and test on Alps.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-systems","title":"Supported systems","text":"<p>Arbor is well-optimised for both CPU and GPU executation and users of systems with and without accelerators, so we will provide it for the following platforms:</p> <ul> <li>multicore: <code>zen2</code>/<code>zen3</code></li> <li><code>gh200</code></li> </ul> <p>supported platforms</p> <p>Supported targets on Alps are currently <code>zen2</code>, <code>zen3</code>, <code>a100</code>, <code>mi200</code>, and <code>gh200</code>.</p> <p>For more information, see the internal CSCS confluence. Also, information about which targets are available on which vClusters, see the <code>config.yaml</code>.</p>"},{"location":"pkg-application-tutorial/#compilers","title":"Compilers","text":"<p>Arbor is a C++17 libarary that officially supports GCC and Clang, with a Python front end.</p> <p>For this we choose the following compiler versions:</p> target compiler cuda python <code>zen2</code>/<code>zen3</code> <code>gcc@13.2</code> - <code>python@3.11</code> <code>gh200</code> <code>gcc@13.2</code> <code>cuda@12.4</code> <code>python@3.11</code>"},{"location":"pkg-application-tutorial/#packages","title":"Packages","text":"<p>The first step when building an application, use-case or workflow uenv is to determine which specs to add to the list.</p> <p>If the aim was to provide arbor with cuda and Python support enabled, an <code>environments.yaml</code> file that provides a single spec <code>arbor@0.9 +python</code> could be sufficient, e.g.:</p> simple environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich\n  unify: true\n  specs:\n  - arbor@0.9 +python\n  variants:\n  - +mpi\n  - +cuda\n  - cuda_arch=90\n  views:\n    arbor:\n      links: root\n</code></pre> <p>This environment definition will build arbor, with all of its dependencies implicitly concretised by Spack. Such a simple recipe is sometimes sufficient, however one will often need to provide a more detailed set of specs. Reasons for more detailed specs include:</p> <ul> <li>to pin the version of a specific dependency, e.g.:<ul> <li>to ensure that the version of a package is not dependent on which version of Spack is used;</li> <li>to a version that is well supported and tested on the target system;</li> <li>to a version that patches a bug on the target system.</li> </ul> </li> <li>to specialise the spec of a specific depency, e.g.:<ul> <li>with non-default variants that support all features on the target system;</li> <li>with non-default variants that give the best performance on the target system;</li> <li>to use a specific compiler when more than one compiler toolchain is used to build packages in an environment.</li> </ul> </li> <li>to explicitly list all of the dependencies to provide to users in an environment view</li> </ul> <p>The objective for the Arbor uenv is to provide both Arbor and all of the tools and libraries to \"build your own\" Arbor. This requires providing all of the libraries and tools required to download the Arbor source code, run CMake, and build in a file system view.</p> <p>As a starting point, we use the spack package for Arbor. From this we derive a list of dependencies:</p> <ul> <li>direct dependencies like <code>pugixml</code>, <code>fmt</code> and <code>pybind11</code> needed to build Arbor.</li> <li>compiler and languages like <code>python</code> and <code>cuda</code>.</li> </ul>"},{"location":"pkg-application-tutorial/#the-recipe","title":"The Recipe","text":"<p>With requirements in hand, it is now time to write the recipe.</p>"},{"location":"pkg-application-tutorial/#config","title":"Config","text":"<p>There are a few simple choices to make when writing the <code>config.yaml</code> file:</p> <code>name</code> <p>Keep it simple, we choose <code>arbor</code>.</p> <p>Tip</p> <p>Use the same name on all hardware targets, i.e. use a name like <code>arbor</code> or <code>gromacs</code> instead of <code>arbor-gpu</code> or <code>gromacs-x86</code>. By doing this users can more easily find your uenv on all vClusters - if they are on a system with an x86 CPU, they can assume that the <code>arbor</code> uenv has been built appropriately.</p> <p>The uenv CLI tool also allows users to disambiguate which micro-architecture they require, if on a system that provides versions of a uenv built for multiple uarch:</p> <pre><code>uenv image ls --uarch=gh200 arbor\nuenv image ls --uarch=zen2 arbor\n</code></pre> <code>spack</code> <p>By default use the most recent version of Spack supported by Stackinator. At the time of writing, the most recent version of Spack is <code>v0.21</code>, for which it is recommend to use the <code>releases/v0.21</code> branch, which receives backports of bug fixes while not changing the API or recipe definitions.</p> <p>Warning</p> <p>The <code>develop</code> branch should be avoided for uenv deployed on CSCS clusters unless it is absolutely neccesary.</p> <code>mount</code> <p>Normally application and development uenv go in <code>/user-environment</code> and tools that you might want to use alongside a development or application uenv go in <code>/user-tools</code> (e.g. a debugger). For Arbor, we choose the default <code>/user-environment</code> path.</p> <code>description</code> <p>Keep it simple, fit it on one line.</p> <code>mc</code><code>gh200</code> config.yaml<pre><code>name: arbor\nspack:\n  commit: releases/v0.21\n  repo: https://github.com/spack/spack.git\nstore: /user-environment\ndescription: The Arbor neuroscience simuluation package and its dependencies for multicore systems.\n</code></pre> config.yaml<pre><code>name: arbor\nspack:\n  commit: releases/v0.21\n  repo: https://github.com/spack/spack.git\nstore: /user-environment\ndescription: The Arbor neuroscience simuluation package and its dependencies for Grace-Hopper.\n</code></pre>"},{"location":"pkg-application-tutorial/#compilers_1","title":"Compilers","text":"<p>Based on our requirements above, defining compilers is straightforward.</p> <code>mc</code><code>gh200</code> compilers.yaml<pre><code>bootstrap:\n  spec: gcc@12.3\ngcc:\n  specs:\n  - gcc@13.2\n</code></pre> compilers.yaml<pre><code>bootstrap:\n  spec: gcc@12.3\ngcc:\n  specs:\n  - gcc@13.2\n</code></pre>"},{"location":"pkg-application-tutorial/#environments","title":"Environments","text":"<p>The environment definitions include the specs that we want to provide to end users, and the selected <code>cuda</code> and <code>python</code> versions where application.</p> <code>mc</code><code>gh200</code> environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich@8.1.29\n  unify: true\n  specs:\n  # arbor\n  - arbor@0.9 +python\n  # build tools\n  - cmake\n  - googletest\n  - ninja\n  - python@3.11\n  # C++ dependencies\n  - fmt\n  - pugixml\n  - nlohmann-json\n  - random123\n  # python packages\n  - py-mpi4py\n  - py-numpy\n  - py-pip\n  - py-pybind11\n  # etc\n  - osu-micro-benchmarks\n  variants:\n  - +mpi\n  packages:\n  - diffutils\n  - gettext\n  - gmake\n  - libxml2\n  - perl\n  views:\n    arbor:\n      link: roots\n    develop:\n      link: roots\n      exclude: [arbor]\n</code></pre> environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich@8.1.29\n      gpu: cuda\n  unify: true\n  specs:\n  # arbor\n  - arbor@0.9 +python\n  # build tools\n  - cuda@12.4\n  - cmake\n  - googletest\n  - ninja\n  - python@3.11\n  # C++ dependencies\n  - fmt\n  - pugixml\n  - nlohmann-json\n  - random123\n  # python packages\n  - py-mpi4py\n  - py-numpy\n  - py-pip\n  - py-pybind11\n  # etc\n  - osu-micro-benchmarks\n  variants:\n  - +mpi\n  - +cuda\n  - cuda_arch=90\n  packages:\n  - diffutils\n  - gettext\n  - gmake\n  - libxml2\n  - perl\n  views:\n    arbor:\n      link: roots\n    develop:\n      link: roots\n      exclude: [arbor]\n</code></pre> <p>variants</p> <p>Environments on GH200 will typically have the following variants set:</p> <ul> <li><code>+cuda</code> sets that variant for all that support it, required for NVIDIA GPU builds.</li> <li><code>cuda_arch=90</code> is required for <code>gh200</code> (use <code>cuda_arch=80</code> for the <code>a100</code> nodes)</li> </ul> <p>views and roots</p> <p>Always use <code>view:link:roots</code> if possible to filter which packages are added to views. The default <code>all</code> setting and also the <code>run</code> setting can add a lot of packages that were not explicitly in the list of your uenv's specs.</p> <p>Packages in the view can lead to conflicts, which can be avoided by only including packages that are strictly required. For example, if a view has a common dependency like <code>libssl</code> in its <code>/lib</code> path, and <code>LD_LIBRARY_PATH</code> is set, system CLI tools like <code>git</code> can crash because the link against the <code>libssl</code> in the uenv at runtime.</p>"},{"location":"pkg-application-tutorial/#modules","title":"Modules","text":"<p>We add a module file, which controls which modules are provided by the uenv. This is because some users might want modules, and it doesn't hurt to provide them (this is a weak reason, and we accept that we will be on the hook for supporting them for users who incorporate them into their workflows).</p> <p>Info</p> <p>If you don't need to provide modules, set <code>modules: False</code> in <code>config.yaml</code>.</p> mcgh200 modules.yaml<pre><code>modules:\n  # Paths to check when creating modules for all module sets\n  prefix_inspections:\n    bin:\n      - PATH\n    lib:\n      - LD_LIBRARY_PATH\n    lib64:\n      - LD_LIBRARY_PATH\n\n  default:\n    arch_folder: false\n    # Where to install modules\n    roots:\n      tcl: /user-environment/modules\n    tcl:\n      all:\n        autoload: none\n      hash_length: 0\n      exclude_implicits: true\n      exclude: ['%gcc@7.5.0', 'gcc %gcc@7.5.0']\n      projections:\n        all: '{name}/{version}'\n</code></pre> modules.yaml<pre><code>modules:\n  # Paths to check when creating modules for all module sets\n  prefix_inspections:\n    bin:\n      - PATH\n    lib:\n      - LD_LIBRARY_PATH\n    lib64:\n      - LD_LIBRARY_PATH\n\n  default:\n    arch_folder: false\n    # Where to install modules\n    roots:\n      tcl: /user-environment/modules\n    tcl:\n      all:\n        autoload: none\n      hash_length: 0\n      exclude_implicits: true\n      exclude: ['%gcc@7.5.0', 'gcc %gcc@7.5.0']\n      projections:\n        all: '{name}/{version}'\n</code></pre>"},{"location":"pkg-application-tutorial/#testing","title":"Testing","text":"<p>Failure</p> <p>write reframe tests.</p>"},{"location":"pkg-application-tutorial/#deployment","title":"Deployment","text":""},{"location":"pkg-application-tutorial/#configuring-the-pipeline","title":"Configuring the pipeline","text":"<p>The target systems for deploying the Arbor uenv to users are Eiger (<code>zen2</code>) and Santis (<code>gh200</code>).</p> <p>To enable the CI/CD pipeline to build and deploy the uenv on these systems, update the <code>config.yaml</code> file in the alps-uenv repository:</p> <pre><code>uenvs:\n  arbor:\n    v0.9:\n      recipes:\n        zen2: v0.9/mc\n        gh200: v0.9/gh200\n      deploy:\n        eiger: [zen2]\n        santis: [gh200]\n</code></pre> <p>Tip</p> <p>To test that the pipeline yaml is correctly configured before pushing the changes and making a PR, you can run a basic test with the new uenv:</p> <pre><code>system=santis uarch=gh200 uenv=arbor:v0.9 ./ci/configure-pipeline\nsystem=eiger uarch=zen2 uenv=arbor:v0.9 ./ci/configure-pipeline\n</code></pre> <p>If there are no obvious error messages, you are good to go!</p>"},{"location":"pkg-application-tutorial/#running-the-pipeline","title":"Running the pipeline","text":"<p>To run the pipeline that will automatically build and test your uenv, first create a PR:</p> <ol> <li>Push your changes to a branch (preferably in a fork of the main alps-uenv repository).</li> <li>Open a PR with your changes.</li> </ol> <p>Once the PR is created, the pipeline has to be triggered for each individual combination of uenv/version/uarch/vCluster by using a specially formatted </p> <pre><code>cscs-ci run alps;system=eiger;uarch=zen2;uenv=arbor:v0.9\ncscs-ci run alps;system=santis;uarch=gh200;uenv=arbor:v0.9\n</code></pre>"},{"location":"pkg-application-tutorial/#checking-the-build","title":"Checking the build","text":"<p>Log onto the target system, e.g. <code>santis</code>, and use the <code>uenv image find --build</code> command to search for the build.</p> <pre><code>&gt; uenv image find --build arbor\nuenv/version:tag                        uarch date       id               size\narbor/v0.9:1250847801                   gh200 2024-04-12 89c9a36f21b496a2 3.6GB\narbor/v0.9:1249535229                   gh200 2024-04-11 0a2d82448ecaafd7 3.6GB\n</code></pre> <p>Info</p> <p>The <code>--build</code> flag is required with the <code>find</code> and <code>pull</code> commands to interact with images that have been built by the pipeline, but not yet deployed.</p> <p>Pick the version to pull (if it isn't clear which version to pull, inspect the logs of the CI/CD job that built the image).</p> <pre><code># pull the image using its id\nuenv image pull --build 89c9a36f21b496a2\n\n# then start the image to test it\nuenv image start 89c9a36f21b496a2\n</code></pre>"},{"location":"pkg-application-tutorial/#docs","title":"Docs","text":"<p>Failure</p> <p>Write about how to document.</p>"},{"location":"uenv-cp2k/","title":"CP2K","text":"<p>CP2K version <code>2023.2</code>.</p> <p>An environment that provides the latest version of CP2K, along with the libraries and tools required to build a different or custom version of CP2K.</p> <p>The following environment views are provided:</p> <ul> <li><code>cp2k-scalapack</code>: CP2K, dependencies, and ScaLAPACK as diagonalization library</li> <li><code>cp2k-scalapack-dev</code>: dependencies and ScaLAPACK</li> <li><code>cp2k-elpa</code>: CP2K, dependencies, and ELPA as diagonalization library</li> <li><code>cp2k-elpa-dev</code>: dependencies and ELPA</li> </ul>"},{"location":"uenv-cp2k/#building-a-custom-version-of-cp2k","title":"Building a custom version of CP2K","text":""},{"location":"uenv-cp2k/#using-modules","title":"Using modules","text":"<p>To build your version of CP2K do the following steps:</p> <pre><code># Load the required modules\nmodule load [...]\ncd cp2k\nmkdir build &amp;&amp; cd build\ncmake .. -DCMAKE_BUILD_TYPE=Release -DCP2K_SCALAPACK_VENDOR=MKL -DCP2K_USE_ACCEL=cuda -DCP2K_WITH_GPU=A100\nmake -j20\n</code></pre> <p>See CP2K's README_cmake.md for details.</p>"},{"location":"uenv-cp2k/#using-spack","title":"Using Spack","text":"<pre><code>uenv start cp2k-a100.squashfs\nexport SPACK_SYSTEM_CONFIG_PATH=/user-environment/config/\nspack install cp2k [...]\n</code></pre> <p>See Spack's CP2K package for details.</p>"},{"location":"uenv-gromacs/","title":"GROMACS","text":"<p>supports a100.</p>"},{"location":"uenv-linaro-forge/","title":"Linaro Forge (DDT) debugger","text":"<p>Linaro Forge (formerly known as DDT) allows source-level debugging of Fortran, C, C++ and Python codes. It can be used for debugging serial, multi-threaded (OpenMP), multi-process (MPI) and accelerated (Cuda, OpenACC) programs running on research and production systems, including CSCS Alps system. It can be executed either as a graphical user interface or from the command-line.</p>"},{"location":"uenv-linaro-forge/#usage-notes","title":"Usage notes","text":"<p>The uenv is named <code>linaro-forge</code>, and the available versions on a cluster can be determined using the <code>uenv image find</code> command, for example: <pre><code>&gt; uenv image find linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-10 ea67dbb33801c7c3 342MB\n</code></pre></p> <p>The linaro tools are configured to be mounted in the <code>/user-tools</code> path so that they can be used alongside application and development uenv mounted at <code>user-environment</code>.</p> sidecarstandalone <p>When using alongside another uenv, start a uenv session with both uenv with <code>linaro-forge</code> after the main uenv, to mount the images at the respective <code>/user-environment</code> and <code>/user-tools</code> locations:</p> <pre><code>uenv start prgenv-gnu/24.2:v3 linaro-forge/23.1.2\n\n# test that everything has been mounted correctly\n# (will give warnings if there are problems)\nuenv status\n\nuenv view prgenv-gnu:default\nsource /user-tools/activate\n\n# check that ddt is in the path\nddt --version\n</code></pre> <p>The <code>/user-tools/activate</code> script will make the forge executables available in your environment, and must be run after any other uenv view command.</p> <p>When using the uenv with no other environment mounted, you will need to explicitly set the <code>/user-tools</code> mount point:</p> <pre><code>uenv start linaro-forge/23.1.2:/user-tools\n\nsource /user-tools/activate\n\n# check that ddt is in the path\nddt --version\n</code></pre> <p>The <code>/user-tools/activate</code> script will make the forge executables available in your environment.</p>"},{"location":"uenv-linaro-forge/#getting-started","title":"Getting Started","text":"<p>In order to debug your code on Alps, you need to:</p> <ol> <li>pull the linaro-forge uenv on the target Alps vCluster</li> <li>install the Forge/DDT client on your laptop</li> <li>build an executable with debug flags</li> <li>launch a job with the debugger on Alps.</li> </ol>"},{"location":"uenv-linaro-forge/#pull-the-linaro-forge-uenv-on-the-alps-cluster","title":"Pull the Linaro Forge uenv on the Alps cluster","text":"<p>The first step is to download the latest version of linaro-forge that is available on the cluster. First, SSH into the target system, then use the <code>uenv image find</code> command to list the available versions on the system:</p> <pre><code>&gt; uenv image find linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-10 ea67dbb33801c7c3 342MB\n</code></pre> <p>In this example, there is a single version available. Next we pull the image so that it is available locally. <pre><code>&gt; uenv image pull linaro-forge/23.1.2:latest\n</code></pre></p> <p>It will take a few seconds to download the image. Once complete, check that it was downloaded using the <code>uenv image ls</code> command:</p> <pre><code>&gt; uenv image ls linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-05 ea67dbb33801c7c3 342MB\n</code></pre>"},{"location":"uenv-linaro-forge/#install-the-client-on-your-laptop","title":"Install the client on your laptop","text":"<p>We recommend installing the desktop client on your local workstation/laptop. It can be configured to connect with the debug jobs running on Alps, offering a better user experience compared running remotely with X11 forwarding. The client can be downloaded for a selection of operating systems, via the link above.</p> <p>Once installed, the client needs to be configured to connect to the vCluster on which you are working. First, start the client on your laptop.</p> LinuxMacOS <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>$HOME/linaro/forge/23.0.1/bin/ddt\n</code></pre> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>open /Applications/Linaro\\ Forge\\ Client\\ 23.0.1.app/\n</code></pre> <p>Next, configure a connection to the target system. Open the Remote Launch menu and click on configure then Add. Examples of the settings are below.</p> EigerSantis Field Value Connection <code>eiger</code> Host Name <code>bsmith@ela.cscs.ch bsmith@eiger.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/23.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>$HOME/.ssh/cscs-key</code> Field Value Connection <code>santis</code> Host Name <code>bsmith@ela.cscs.ch bsmith@santis.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/23.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>$HOME/.ssh/cscs-key</code> <p>Some notes on the examples above:</p> <ul> <li>SSH Forwarding via <code>ela.scscs.ch</code> is used to access the cluster.</li> <li>the replace the username <code>bsmith</code> with your CSCS user name that you would normally use to open an SSH connection to CSCS.</li> <li>the Remote Installation Path is a little bit more complicated than</li> <li>the private keys should be the ones generated for CSCS MFA, and this field does not need to be set if you have added the key to your SSH agent.</li> </ul> <p>Once configured, test and save the configuration:</p> <ol> <li>check whether the concfiguration is correct, click <code>Test Remote Launch</code>.</li> <li>Click on <code>ok</code> and <code>close</code> to save the configuration.</li> <li>You can now connect by going to <code>Remote Launch</code> and choose the <code>Alps</code> entry. If the client fails to connect, look at the message, check your ssh configuration and make sure you can ssh without the client.</li> </ol>"},{"location":"uenv-linaro-forge/#setup-the-environment","title":"Setup the environment","text":""},{"location":"uenv-linaro-forge/#build-with-debug-flags","title":"Build with debug flags","text":"<p>Once the uenv is loaded and activated, the program to debug must be compiled with the <code>-g</code> (for cpu) and <code>-G</code> (for gpu) debugging flags. For example, let's build a cuda code with  a user environment:</p> <pre><code>uenv start prgenv-gnu:24.2:v2\nuenv view default\n\n# download the source code\ngit clone https://github.com/sekelle/octree-miniapp.git\ncd o\n\n\n# build the application\nmake -C octree-miniapp.git/\n</code></pre>"},{"location":"uenv-linaro-forge/#launch-the-code-with-the-debugger","title":"Launch the code with the debugger","text":"<p>To use the DDT client with uenv, it must be launched in <code>Manual Launch</code> mode (assuming that it is connected to Alps via <code>Remote Launch</code>):</p> Note <p>the steps below do not manually launch - instead they directly launch using <code>ddt --connect srun ...</code> on the target cluster.</p> on laptopon Alps <p>Start DDT, and connect to the target cluster using the drop down menu for Remote Launch.</p> <p>Then wait for the job to start (see the \"on Alps\" tab).</p> <p>log into the system and launch with the srun command:</p> <pre><code># start a session with both the PE used to build your application\n# and the linaro-forge uenv mounted\nuenv start prgenv-gnu/24.2 linaro-forge/23.1.2\nddt --connect srun -n2 -N2 ./a.out\n</code></pre> <p>Notes on using specific systems:</p> santis <p>Warning</p> <p>Because Santis is not connected to the internet, some environment variables need to be set so that it can connect to the license server.</p> <pre><code>export https_proxy=proxy.cscs.ch:8080\nexport http_proxy=proxy.cscs.ch:8080\nexport no_proxy=\".local, .cscs.ch, localhost, 148.187.0.0/16, 10.0.0.0/8, 172.16.0.0/12\"\n</code></pre> default value of <code>http_proxy</code> <p>By default the <code>https_proxy</code> and <code>http_proxy</code> variables are set to <code>http://proxy.cscs.ch:8080</code>, as the transport is required for some other services to work. You will have to set them for a debugging session.</p> <p>This screenshot shows a debugging session on 12 gpus:</p> <p></p>"},{"location":"uenv-qe/","title":"Quantum ESPRESSO","text":"<p>https://www.quantum-espresso.org/</p> <p>An environment that provides the latest version of Quantum ESPRESSO, along with the libraries and tools required to build a different or custom version of Quantum ESPRESSO. At the moment a GPU-build environment is provided without a ScaLAPACK.</p> <p>The following environment views are provided:</p> <ul> <li>default : QuantumESPRESSO/7.1 itself + dependencies</li> <li>develop : only dependencies</li> </ul> <p>The following modules are provided:</p> <ul> <li>cmake/3.26.3</li> <li>cray-mpich/8.1.25-nvhpc</li> <li>cuda/11.8.0</li> <li>fftw/3.3.10</li> <li>gcc/11.3.0</li> <li>libxc/5.2.3</li> <li>nvhpc/22.11</li> <li>openblas/0.3.23</li> <li>patchelf/0.17.2</li> <li>quantum-espresso/7.1</li> </ul>"},{"location":"uenv-qe/#building-a-custom-version","title":"Building a custom version","text":""},{"location":"uenv-qe/#using-modules","title":"using modules","text":"<p>To build your version of QE do the following steps:</p> <pre><code>module load cmake cray-mpich/8.1.25-nvhpc cuda fftw libxc/5.2.3-nvhpc nvhpc openblas\ncd qe-71-dev\nmkdir build &amp;&amp; cd build\ncmake .. -DQE_ENABLE_OPENMP=1 -DQE_ENABLE_SCALAPACK=0 -DQE_ENABLE_LIBXC=1 -DQE_ENABLE_CUDA=1\nmake -j20\n</code></pre>"},{"location":"uenv-qe/#using-spack","title":"using Spack","text":"<p>todo.</p>"}]}